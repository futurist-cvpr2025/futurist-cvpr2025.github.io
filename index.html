<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="FUTURIST"/>
  <meta property="og:description" content="Advancing Semantic Future Prediction through Multimodal Visual Sequence Transformers"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/overview.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>

  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content=" Semantic Future Prediction, Multimodal Transformers, Masked Visual Modeling">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Advancing Semantic Future Prediction through Multimodal Visual Sequence Transformers</title>
<!--   <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Advancing Semantic Future Prediction through Multimodal Visual Sequence Transformers</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=jif2JYsAAAAJ&hl=en" target="_blank">Efstathios Karypidis</a><sup>1,3</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=B_dKcz4AAAAJ&hl=en" target="_blank">Ioannis Kakogeorgiou</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.fr/citations?user=7atfg7EAAAAJ&hl=en" target="_blank">Spyros Gidaris</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=xCPoT4EAAAAJ&hl=en" target="_blank">Nikos Komodakis</a><sup>1,4,5</sup>
              </span>
            </div>

            <div class="is-size-6 publication-authors">
                  <span>1. Archimedes/Athena RC</span> | 
                  <span>2. valeo.ai</span> | 
                  <span>3. National Technical University of Athens</span> | 
                  <span>4. University of Crete</span> | 
                  <span>5. IACM-Forth</span>
            </div>

            <div class="column has-text-centered">
              <div class="is-size-5 publication-authors">
                <span class="author-block"><b>CVPR 2025</b></span>
              </div>
            </div>
            

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- CVPR2 25 Open Access -->
                      <span class="link-block">
                        <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Karypidis_Advancing_Semantic_Future_Prediction_through_Multimodal_Visual_Sequence_Transformers_CVPR_2025_paper.html" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                        </a>
                      </span>

                       <!-- ArXiv abstract Link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2501.08303" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="ai ai-arxiv"></i>
                        </span>
                        <span>arXiv</span>
                        </a>
                      </span>

                    <!-- Supplementary PDF link -->
<!--                     <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/Sta8is/FUTURIST" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <span class="link-block">
                  <a href="https://huggingface.co/Sta8is/FUTURIST/blob/main/futurist.ckpt" target="_blank"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon" style="line-height:1;">
                      <span aria-hidden="true" style="font-size:18px; display:inline-block;">ðŸ¤—</span>
                    </span>
                    <span>Model Weights</span>
                  </a>
                </span>
                
                <!-- Keep Academicons if you use it elsewhere; not required for the emoji -->
                <link rel="stylesheet"
                      href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.4/css/academicons.min.css" />

               
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>



  
<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/videos/banner_video.gif" alt="Teaser" style="width: 100%; height: auto;">
      <h2 class="subtitle has-text-centered">
        Our framework predicts future semantic segmentation and depth maps using a multimodal transformer architecture. Leveraging masked visual modeling and cross-modal fusion, it excels in future semantic prediction, achieving state-of-the-art results in both tasks. Context frames are highlighted with a <span style="color:#00ff00;font-weight:600;">green</span> border, while generated frames are highlighted with a <span style="color:#0000ff;font-weight:600;">blue</span> border.
      </h2>
    </div>
  </div>
</section>
  
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
<!--       <div class="column is-four-fifths"> -->
      <div class="column is-full">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Semantic future prediction is important for autonomous systems navigating dynamic environments. This paper introduces FUTURIST, a method for multimodal future semantic prediction that uses a unified and efficient visual sequence transformer architecture. Our approach incorporates a multimodal masked visual modeling objective and a novel masking mechanism designed for multimodal training. This allows the model to effectively integrate visible information from various modalities, improving prediction accuracy. Additionally, we propose a VAE-free hierarchical tokenization process, which reduces computational complexity, streamlines the training pipeline, and enables end-to-end training with high-resolution, multimodal inputs. We validate FUTURIST on the Cityscapes dataset, demonstrating state-of-the-art performance in future semantic segmentation for both short- and mid-term forecasting.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {inlineMath: [['\\(', '\\)']]}
    });
</script>


<!-- FUTURIST Section -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
<!--       <div class="column is-four-fifths"> -->
      <div class="column is-full">
        <center><h2 class="title is-3"> FUTURIST: End-to-End Trainable Multimodal Visual Sequence Transformer</h2></center>
        <div class="content has-text-justified">
          <p>
            Our framework employs modality-specific embedders to map inputs into tokens. Then, these token embeddings are fused through token-wise concatenation to form the combined input embedding \(\mathrm{Z}\). Next, a masked transformer processes \(\mathrm{Z}\), capturing spatiotemporal dependencies between modalities, and outputs \(\mathrm{\tilde{Z}}\). Finally, modality-specific decoders produce future frame predictions for each modality based on \(\mathrm{\tilde{Z}}\), enabling efficient and accurate multimodal semantic future prediction within a unified architecture. Tokens are shown in 2D (not flattened) for visualization purposes.          
          </p>
        </div>
        <!-- Image -->
        <div class="has-text-centered">
          <img src="static/images/overview.png" alt="FUTURIST Architecture" style="width: 100%; height: auto;">
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End FUTURIST Section -->



<!-- Main Components Section -->
<section class="section" id="Main-Components">
  <div class="container is-max-desktop">
    <center><h2 class="title">Main Components</h2></center>
    <div class="content has-text-justified">
      <p>
        <strong>VAE-free Multimodal Embedder</strong>: Our framework employs a novel tokenization approach that eliminates the dependency on VAE-based tokenizers commonly used in video generation models. This lightweight, hierarchical two-stage embedding process enables efficient end-to-end training while maintaining computational efficiency for multimodal inputs.
      </p>
    </div>
    <!-- Image Section -->
      <div class="has-text-centered">
        <img src="static/images/hiera_embedder.png" alt="Modality Embedder" style="width: 80%; height: auto;">
      </div>
    <div class="content has-text-justified">
      <p>
        <strong>Cross Modal Fusion</strong>: Our approach integrates multiple semantic modalities by merging tokens across different modalities that share the same spatiotemporal location. We explored two fusion strategies: CONCAT (concatenating embeddings) and ADD (summing embeddings). As shown in the first table, CONCAT consistently outperforms ADD across all metrics, making it our default choice. The second table demonstrates that our multimodal fusion approach achieves superior performance when compared to separate tokens approach under the same compute budget.
      </p>
    </div>
<!--     <div class="columns">
      <div class="column is-half has-text-centered">
        <img src="static/images/Modal_fusion1.png" alt="Modality Fusion Strategy Table" style="width: 100%; height: auto;">
      </div>
      <div class="column is-half has-text-centered">
        <img src="static/images/Modal_fusion2.png" alt="Multimodal Fusion Training Table" style="width: 100%; height: auto;">
      </div>
    </div> -->
    <div class="has-text-centered" style="margin-bottom: 20px;">
      <img src="static/images/Modal_fusion1.png" alt="Modality Fusion Strategy Table1" style="width: 80%; height: auto;">
    </div>

    <div class="has-text-centered" style="margin-bottom: 20px;">
      <img src="static/images/Modal_fusion2.png" alt="Modality Fusion Strategy Table2" style="width: 80%; height: auto;">
    </div>


    <div class="content has-text-justified" style="margin-top:2rem;">
      <p>
        <strong>Multimodal Masking Strategy</strong>: We investigated several masking strategies to improve multimodal learning efficiency and accuracy. The <strong>Partially-shared and exclusive masking approach</strong> enforces learning cross-modal dependencies by selectively masking tokens across modalities, enabling the model to better exploit synergy between modalities.
      </p>
    </div>
    <div class="has-text-centered" style="margin-bottom: 20px;">
      <img src="static/images/masking_strategies.png" alt="Multimodal Masking Strategy Illustration" style="max-width: 80%; height: auto;">
    </div>
<!--     <div class="has-text-centered">
      <img src="static/images/masking_strategies_tab.png" alt="Multimodal Masking Strategy Table" style="max-width: 100%; height: auto;">
    </div> -->

    <div class="content has-text-justified" style="margin-top: 2rem;">
      <p>
        <strong>Scaling Training Epochs</strong>: Longer training significantly improves our model's performance for both segmentation and depth prediction. As shown in the figure, mIoU and AbsRel metrics improve steadily with increased epochs for short-term and mid-term predictions.
      </p>
    </div>
    <div class="has-text-centered" style="margin-bottom: 20px;">
      <img src="static/images/scaling.png" alt="Scaling Training Epochs" style="max-width: 80%; height: auto;">
    </div>

    <div class="content has-text-justified" style="margin-top: 2rem;">
      <p>
        <strong>Synergistic Joint Prediction</strong>: Exploiting multimodal synergy boosts accuracy beyond single-modality models. The table demonstrates that the combined segmentation and depth training outperforms separate models, highlighting the strength of our unified approach.
      </p>
    </div>
    <div class="has-text-centered">
      <img src="static/images/synergistic_tab.png" alt="Synergistic Joint Prediction Table" style="max-width: 80%; height: auto;">
    </div>
  </div>
</section>
<!-- End Main Components Section -->


<!-- Comparison with prior works -->
<section class="section" id="comparison-prior-works">
  <div class="container is-max-desktop">
    <center><h2 class="title">Comparison with prior works</h2></center>

    <div class="content has-text-justified">
      <p>
        We achieve state-of-the-art results for future semantic prediction. Here we compare against prior methods, with Oracle representing the upper bound performance. We also include the weak Copy-Last baseline and introduce Vista as stronger baseline.
    </div>

    <div class="has-text-centered">
      <!-- Replace the src with your table image path -->
      <img src="static/images/Results.png" alt="Comparison with prior works table" style="width: 50%; height: auto;">
    </div>
  </div>
</section>
<!-- End Comparison with prior works -->


<!-- GIF carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <center><h2 class="title is-3">Autoregressive Rollouts</h2></center>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-gif1">
          <img src="static/videos/carousel1.gif" alt="GIF 1" style="width: 100%; height: auto; display: block;">
        </div>
        <div class="item item-gif2">
          <img src="static/videos/carousel2.gif" alt="GIF 2" style="width: 100%; height: auto; display: block;">
        </div>
        <div class="item item-gif3">
          <img src="static/videos/carousel3.gif" alt="GIF 3" style="width: 100%; height: auto; display: block;">
        </div>
        <div class="item item-gif4">
          <img src="static/videos/carousel4.gif" alt="GIF 4" style="width: 100%; height: auto; display: block;">
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End GIF carousel -->









<!--BibTex citation -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
      <center><h2 class="title">Citation</h2></center>
      <div style="position: relative; background-color: #f5f5f5; padding: 10px; border-radius: 5px; border: 1px solid #ddd;">
          <button onclick="copyToClipboard()" 
                  style="position: absolute; right: 10px; top: 10px; background-color: #007bff; color: #fff; border: none; padding: 5px 10px; border-radius: 5px; cursor: pointer;"
                  onmouseover="this.style.backgroundColor='#0056b3'" 
                  onmouseout="this.style.backgroundColor='#007bff'">
              Copy
          </button>
          <pre style="margin: 0; padding-right: 100px; overflow-x: auto;">
            <code id="code-snippet">
@InProceedings{Karypidis_2025_CVPR,
author    = {Karypidis, Efstathios and Kakogeorgiou, Ioannis and Gidaris, Spyros and Komodakis, Nikos},
title     = {Advancing Semantic Future Prediction through Multimodal Visual Sequence Transformers},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month     = {June},
year      = {2025},
pages     = {3793-3803}
}
            </code></pre>
      </div>  
  </div>
  <script>
      function copyToClipboard() {
          var code = document.getElementById("code-snippet").innerText;
          navigator.clipboard.writeText(code).then(function() {
              alert('Citation copied to clipboard!');
          }, function(err) {
              alert('Failed to copy citation: ', err);
          });
      }
  </script>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from theÂ <a href="https://nerfies.github.io" target="_blank">Nerfies</a>Â project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
